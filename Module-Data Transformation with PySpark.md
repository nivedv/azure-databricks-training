# PySpark Tutorial: Adventure Works Sales Analysis

## By Nived Varma - Microsoft Certified Trainer

## Azure Databricks Implementation

## Dataset Overview: Your E-commerce Success Story! üìà

We're working with **Adventure Works** sales data that shows incredible growth:

- **2019**: 1,201 transactions ‚Üí $3.86M revenue
- **2020**: 2,733 transactions ‚Üí $6.37M revenue
- **2021**: 28,784 transactions ‚Üí $10.69M revenue

**Data Schema:**

```
SalesOrderNumber | SalesOrderLineNumber | OrderDate | CustomerName | Email | Item | Quantity | UnitPrice | Tax
```

---

## Setup: Prepare Your Data in Databricks üöÄ

```python
# Databricks notebook source
# Create directory for our data files (if not created earlier)
dbutils.fs.mkdirs("/FileStore/spark_lab/")

# Import required functions
from pyspark.sql.functions import *
from pyspark.sql.types import *

# Define schema for consistent data loading
orderSchema = StructType([
    StructField("SalesOrderNumber", StringType()),
    StructField("SalesOrderLineNumber", IntegerType()),
    StructField("OrderDate", DateType()),
    StructField("CustomerName", StringType()),
    StructField("Email", StringType()),
    StructField("Item", StringType()),
    StructField("Quantity", IntegerType()),
    StructField("UnitPrice", FloatType()),
    StructField("Tax", FloatType())
])

# Load all CSV files at once (Databricks magic!)
df = spark.read.format('csv').schema(orderSchema).load("/FileStore/spark_lab/*.csv")

print(f"üìä Total records loaded: {df.count()}")
display(df.limit(10))
```

---

## 1. SELECT & FILTER: Finding Your Best Customers üéØ

### Real-World Scenario: "Who are our high-value customers and what do they buy?"

```python
# COMMAND ----------

# EXAMPLE 1: SELECT specific columns for customer analysis
customer_orders = df.select("CustomerName", "Email", "Item", "UnitPrice", "Quantity")
display(customer_orders.limit(10))

# EXAMPLE 2: FILTER high-value transactions (>$1000)
high_value_sales = df.filter(col("UnitPrice") > 1000)
print(f"üèÜ High-value transactions: {high_value_sales.count()}")
display(high_value_sales.select("CustomerName", "Item", "UnitPrice").orderBy(desc("UnitPrice")))

# EXAMPLE 3: Multiple filter conditions - Premium bike customers
premium_customers = df.filter(
    (col("UnitPrice") > 2000) &
    (col("Item").contains("Mountain")) &
    (col("Quantity") >= 1)
)

print(f"üíé Premium mountain bike customers: {premium_customers.count()}")
display(premium_customers.select("CustomerName", "Item", "UnitPrice", "Quantity"))

# COMMAND ----------

# EXAMPLE 4: SELECT with calculated columns and conditional logic
sales_analysis = df.select(
    "CustomerName",
    "Item",
    "UnitPrice",
    "Quantity",
    (col("UnitPrice") * col("Quantity")).alias("Revenue"),
    when(col("UnitPrice") > 2000, "Premium")
    .when(col("UnitPrice") > 1000, "Mid-Range")
    .otherwise("Standard").alias("PriceCategory")
)

display(sales_analysis.filter(col("Revenue") > 3000).orderBy(desc("Revenue")))

# COMMAND ----------

# EXAMPLE 5: Advanced filtering with date ranges
from datetime import datetime

recent_sales = df.filter(
    (col("OrderDate") >= "2021-01-01") &
    (col("OrderDate") <= "2021-12-31") &
    (col("UnitPrice") > 500)
)

print(f"üìà Recent high-value sales (2021): {recent_sales.count()}")
display(recent_sales.select("OrderDate", "CustomerName", "Item", "UnitPrice").orderBy("OrderDate"))
```

**Key Takeaways:**

- No SparkSession needed in Databricks - it's pre-configured!
- Use `display()` instead of `show()` for better Databricks visualization
- `col()` function for column references
- Chain conditions with `&` (and) and `|` (or)

---

## 2. GROUP BY: Discovering Sales Patterns üìä

### Real-World Scenario: "What are our best-selling products and customer segments?"

```python
# COMMAND ----------

# EXAMPLE 1: Total revenue by product
product_revenue = df.groupBy("Item").agg(
    sum(col("UnitPrice") * col("Quantity")).alias("TotalRevenue"),
    count("SalesOrderNumber").alias("OrderCount"),
    avg("UnitPrice").alias("AvgPrice"),
    sum("Quantity").alias("TotalUnitsSold")
).orderBy(desc("TotalRevenue"))

print("üèÜ TOP PRODUCTS BY REVENUE:")
display(product_revenue.limit(15))

# COMMAND ----------

# EXAMPLE 2: Monthly sales trends using SQL magic
df.createOrReplaceTempView("salesorder")

# MAGIC %sql
# MAGIC SELECT
# MAGIC   year(OrderDate) as OrderYear,
# MAGIC   month(OrderDate) as OrderMonth,
# MAGIC   sum(UnitPrice * Quantity) as MonthlyRevenue,
# MAGIC   count(SalesOrderNumber) as TransactionCount,
# MAGIC   count(distinct CustomerName) as UniqueCustomers
# MAGIC FROM salesorder
# MAGIC GROUP BY year(OrderDate), month(OrderDate)
# MAGIC ORDER BY OrderYear, OrderMonth

# COMMAND ----------

# EXAMPLE 3: Customer segmentation analysis
customer_segments = df.groupBy("CustomerName", "Email").agg(
    sum(col("UnitPrice") * col("Quantity")).alias("TotalSpent"),
    count("SalesOrderNumber").alias("OrderFrequency"),
    countDistinct("Item").alias("ProductVariety"),
    max("OrderDate").alias("LastOrderDate"),
    avg("UnitPrice").alias("AvgOrderValue")
).withColumn("CustomerSegment",
    when(col("TotalSpent") > 10000, "VIP")
    .when(col("TotalSpent") > 5000, "Premium")
    .when(col("TotalSpent") > 1000, "Regular")
    .otherwise("New")
).orderBy(desc("TotalSpent"))

print("üéØ CUSTOMER SEGMENTATION:")
display(customer_segments.limit(20))

# COMMAND ----------

# EXAMPLE 4: Product category analysis with advanced grouping
product_category_sales = df.withColumn("ProductCategory",
    when(col("Item").contains("Mountain"), "Mountain Bikes")
    .when(col("Item").contains("Road"), "Road Bikes")
    .when(col("Item").contains("Touring"), "Touring Bikes")
    .otherwise("Other")
).groupBy("ProductCategory").agg(
    sum(col("UnitPrice") * col("Quantity")).alias("CategoryRevenue"),
    avg("UnitPrice").alias("AvgProductPrice"),
    count("*").alias("SalesCount"),
    countDistinct("CustomerName").alias("UniqueCustomers")
).orderBy(desc("CategoryRevenue"))

print("üö¥ PRODUCT CATEGORY PERFORMANCE:")
display(product_category_sales)

# COMMAND ----------

# EXAMPLE 5: Yearly comparison using SQL
# MAGIC %sql
# MAGIC SELECT
# MAGIC   year(OrderDate) as OrderYear,
# MAGIC   sum(UnitPrice * Quantity) as TotalSales,
# MAGIC   count(distinct CustomerName) as UniqueCustomers,
# MAGIC   count(SalesOrderNumber) as TotalOrders,
# MAGIC   avg(UnitPrice * Quantity) as AvgOrderValue
# MAGIC FROM salesorder
# MAGIC GROUP BY year(OrderDate)
# MAGIC ORDER BY OrderYear
```

---

## 3. JOINS & UNIONS: Connecting the Data Story üîó

### Real-World Scenario: "Customer journey analysis and data integration"

```python
# COMMAND ----------

# First, let's separate data by year and add year columns
sales_2019 = df.filter(year(col("OrderDate")) == 2019).withColumn("DataYear", lit(2019))
sales_2020 = df.filter(year(col("OrderDate")) == 2020).withColumn("DataYear", lit(2020))
sales_2021 = df.filter(year(col("OrderDate")) == 2021).withColumn("DataYear", lit(2021))

print("üìä DATA DISTRIBUTION BY YEAR:")
print(f"2019 records: {sales_2019.count()}")
print(f"2020 records: {sales_2020.count()}")
print(f"2021 records: {sales_2021.count()}")

# UNION: Combine all years into one dataset
all_sales = sales_2019.union(sales_2020).union(sales_2021)
print(f"Total combined records: {all_sales.count()}")

# COMMAND ----------

# EXAMPLE 1: Customer behavior by year
customer_yearly_summary = all_sales.groupBy("Email", "CustomerName", "DataYear").agg(
    sum(col("UnitPrice") * col("Quantity")).alias("YearlySpending"),
    count("SalesOrderNumber").alias("YearlyOrders"),
    countDistinct("Item").alias("ProductVariety")
)

display(customer_yearly_summary.orderBy("Email", "DataYear"))

# COMMAND ----------

# EXAMPLE 2: INNER JOIN - Find customers who bought in multiple years
customers_2019 = customer_yearly_summary.filter(col("DataYear") == 2019).select("Email", "CustomerName")
customers_2020 = customer_yearly_summary.filter(col("DataYear") == 2020).select("Email")
customers_2021 = customer_yearly_summary.filter(col("DataYear") == 2021).select("Email")

# Loyal customers (bought in all 3 years)
loyal_customers = customers_2019.join(customers_2020, "Email", "inner") \
                                .join(customers_2021, "Email", "inner")

print(f"üèÜ LOYAL CUSTOMERS (3+ YEARS): {loyal_customers.count()}")
display(loyal_customers)

# COMMAND ----------

# EXAMPLE 3: LEFT JOIN - Customer lifecycle analysis
customer_lifecycle = customer_yearly_summary.filter(col("DataYear") == 2019) \
    .select("Email", "CustomerName", col("YearlySpending").alias("Spending_2019")) \
    .join(
        customer_yearly_summary.filter(col("DataYear") == 2020)
        .select("Email", col("YearlySpending").alias("Spending_2020")),
        "Email", "left"
    ).join(
        customer_yearly_summary.filter(col("DataYear") == 2021)
        .select("Email", col("YearlySpending").alias("Spending_2021")),
        "Email", "left"
    ).withColumn("CustomerJourney",
        when((col("Spending_2019").isNotNull()) &
             (col("Spending_2020").isNotNull()) &
             (col("Spending_2021").isNotNull()), "3-Year Customer")
        .when((col("Spending_2019").isNotNull()) &
              (col("Spending_2020").isNotNull()), "2-Year Customer")
        .when(col("Spending_2019").isNotNull(), "2019 Only")
        .otherwise("Unknown")
    )

print("üìà CUSTOMER LIFECYCLE ANALYSIS:")
display(customer_lifecycle.groupBy("CustomerJourney").count().orderBy(desc("count")))

# COMMAND ----------

# EXAMPLE 4: Product performance pivot analysis
product_yearly_performance = all_sales.groupBy("Item", "DataYear").agg(
    sum(col("UnitPrice") * col("Quantity")).alias("YearlyRevenue"),
    sum("Quantity").alias("UnitsSold")
)

# Use SQL for easier pivot
product_yearly_performance.createOrReplaceTempView("product_performance")

# MAGIC %sql
# MAGIC SELECT
# MAGIC   Item,
# MAGIC   COALESCE(`2019`, 0) as Revenue_2019,
# MAGIC   COALESCE(`2020`, 0) as Revenue_2020,
# MAGIC   COALESCE(`2021`, 0) as Revenue_2021,
# MAGIC   (COALESCE(`2019`, 0) + COALESCE(`2020`, 0) + COALESCE(`2021`, 0)) as TotalRevenue
# MAGIC FROM (
# MAGIC   SELECT Item, DataYear, YearlyRevenue
# MAGIC   FROM product_performance
# MAGIC ) PIVOT (
# MAGIC   sum(YearlyRevenue)
# MAGIC   FOR DataYear IN (2019, 2020, 2021)
# MAGIC )
# MAGIC ORDER BY TotalRevenue DESC
# MAGIC LIMIT 20

# COMMAND ----------

# EXAMPLE 5: Advanced customer cohort analysis using joins
first_purchase = all_sales.groupBy("Email").agg(
    min("OrderDate").alias("FirstPurchaseDate"),
    first("CustomerName").alias("CustomerName")
).withColumn("CohortYear", year(col("FirstPurchaseDate")))

# Join sales data with first purchase info
cohort_analysis = all_sales.join(first_purchase, "Email") \
    .withColumn("OrderYear", year(col("OrderDate"))) \
    .withColumn("YearsSinceFirst", col("OrderYear") - col("CohortYear")) \
    .groupBy("CohortYear", "YearsSinceFirst").agg(
        countDistinct("Email").alias("ActiveCustomers"),
        sum(col("UnitPrice") * col("Quantity")).alias("Revenue")
    ).orderBy("CohortYear", "YearsSinceFirst")

display(cohort_analysis)
```

---

## 4. UDFs (User-Defined Functions): Custom Business Logic üõ†Ô∏è

### Real-World Scenario: "Advanced product categorization & customer scoring"

```python
# COMMAND ----------

from pyspark.sql.functions import udf
from pyspark.sql.types import StringType, IntegerType, DoubleType

# EXAMPLE 1: Advanced product categorization UDF
def categorize_product_advanced(item_name):
    """Advanced product categorization with bike specifications"""
    if not item_name:
        return "Unknown"

    item_lower = item_name.lower()

    # Mountain bikes
    if "mountain" in item_lower:
        if "100" in item_name:
            return "Mountain-Premium"
        elif "200" in item_name:
            return "Mountain-Sport"
        else:
            return "Mountain-Standard"

    # Road bikes
    elif "road" in item_lower:
        if "150" in item_name:
            return "Road-Premium"
        elif "250" in item_name or "550" in item_name:
            return "Road-Sport"
        elif "650" in item_name:
            return "Road-Standard"
        else:
            return "Road-Entry"

    # Touring bikes
    elif "touring" in item_lower:
        return "Touring"

    else:
        return "Specialty"

# Register the UDF
categorize_product_udf = udf(categorize_product_advanced, StringType())

# Apply UDF to create enriched dataset
enriched_sales = df.withColumn(
    "ProductCategory", categorize_product_udf(col("Item"))
).withColumn(
    "Revenue", col("UnitPrice") * col("Quantity")
)

print("üîß ENRICHED SALES DATA WITH UDF:")
display(enriched_sales.select("Item", "ProductCategory", "UnitPrice", "Revenue").limit(15))

# COMMAND ----------

# EXAMPLE 2: Customer loyalty scoring UDF
def calculate_loyalty_score(total_spent, order_count, product_variety):
    """Calculate customer loyalty score based on multiple factors"""
    if not all([total_spent, order_count, product_variety]):
        return 0

    # Convert to ensure we have proper numeric values
    total_spent = float(total_spent) if total_spent else 0
    order_count = int(order_count) if order_count else 0
    product_variety = int(product_variety) if product_variety else 0

    # Base score from spending (max 50 points)
    spending_score = min([50, total_spent / 1000 * 10])

    # Frequency score (max 30 points)
    frequency_score = min([30, order_count * 3])

    # Variety score (max 20 points) - customers who try different products
    variety_score = min([20, product_variety * 5])

    return int(spending_score + frequency_score + variety_score)

loyalty_score_udf = udf(calculate_loyalty_score, IntegerType())

# Apply loyalty scoring
customer_metrics = df.groupBy("Email", "CustomerName").agg(
    sum(col("UnitPrice") * col("Quantity")).alias("TotalSpent"),
    count("SalesOrderNumber").alias("OrderCount"),
    countDistinct("Item").alias("ProductVariety")
).withColumn(
    "LoyaltyScore",
    loyalty_score_udf(col("TotalSpent"), col("OrderCount"), col("ProductVariety"))
).withColumn(
    "CustomerTier",
    when(col("LoyaltyScore") >= 80, "Diamond")
    .when(col("LoyaltyScore") >= 60, "Platinum")
    .when(col("LoyaltyScore") >= 40, "Gold")
    .when(col("LoyaltyScore") >= 20, "Silver")
    .otherwise("Bronze")
)

print("üíé CUSTOMER LOYALTY ANALYSIS:")
display(customer_metrics.orderBy(desc("LoyaltyScore")).limit(20))

# COMMAND ----------

# EXAMPLE 3: Price tier classification UDF
def classify_price_tier(unit_price, product_category):
    """Classify price tier based on product category"""
    if not unit_price or not product_category:
        return "Unknown"

    if "Mountain-Premium" in product_category:
        if unit_price > 3000:
            return "Ultra-Premium"
        elif unit_price > 2000:
            return "Premium"
        else:
            return "Standard"
    elif "Road" in product_category:
        if unit_price > 2000:
            return "Premium"
        elif unit_price > 1000:
            return "Mid-Range"
        else:
            return "Entry-Level"
    else:
        if unit_price > 1500:
            return "Premium"
        elif unit_price > 500:
            return "Mid-Range"
        else:
            return "Budget"

price_tier_udf = udf(classify_price_tier, StringType())

# Apply price tier classification
product_analysis = enriched_sales.withColumn(
    "PriceTier", price_tier_udf(col("UnitPrice"), col("ProductCategory"))
)

display(product_analysis.select("Item", "ProductCategory", "UnitPrice", "PriceTier").limit(15))

# COMMAND ----------

# EXAMPLE 4: Email domain analysis UDF for B2B vs B2C segmentation
def extract_email_domain_type(email):
    """Extract domain type from email for B2B vs B2C analysis"""
    if not email or "@" not in email:
        return "Unknown"

    domain = email.split("@")[1].lower()

    # Consumer email domains
    consumer_domains = ["gmail.com", "yahoo.com", "hotmail.com", "outlook.com", "aol.com"]

    if domain in consumer_domains:
        return "Consumer"
    elif domain == "adventure-works.com":
        return "Internal"
    else:
        return "Business"

email_domain_udf = udf(extract_email_domain_type, StringType())

# Customer segmentation by email domain
customer_segmentation = df.withColumn(
    "CustomerType", email_domain_udf(col("Email"))
).withColumn(
    "ProductCategory", categorize_product_udf(col("Item"))
).groupBy("CustomerType", "ProductCategory").agg(
    sum(col("UnitPrice") * col("Quantity")).alias("TotalRevenue"),
    count("SalesOrderNumber").alias("TransactionCount"),
    avg(col("UnitPrice") * col("Quantity")).alias("AvgOrderValue")
).orderBy("CustomerType", desc("TotalRevenue"))

print("üè¢ B2B vs B2C ANALYSIS BY PRODUCT CATEGORY:")
display(customer_segmentation)

# COMMAND ----------

# EXAMPLE 5: Performance comparison - UDF vs Built-in functions
print("‚ö° PERFORMANCE COMPARISON:")

# Using UDF (slower but more flexible)
udf_result = df.withColumn("Category_UDF", categorize_product_udf(col("Item")))

# Using built-in functions (faster)
builtin_result = df.withColumn("Category_Builtin",
    when(col("Item").contains("Mountain"), "Mountain")
    .when(col("Item").contains("Road"), "Road")
    .when(col("Item").contains("Touring"), "Touring")
    .otherwise("Other")
)

print("‚úÖ Use UDFs for complex business logic")
print("‚ö° Use built-in functions for simple transformations (much faster!)")

display(builtin_result.select("Item", "Category_Builtin").limit(10))
```

---

## üöÄ PUTTING IT ALL TOGETHER: Complete Business Analysis

Let's solve a complete business problem combining all techniques:

```python
# COMMAND ----------

print("üéØ ADVENTURE WORKS 2022 MARKETING STRATEGY ANALYSIS")
print("=" * 60)

# Step 1: Data quality check (like your original code)
duplicate_counts = df.groupBy("SalesOrderNumber").count().filter(col("count") > 1).count()
print(f"üìã Data Quality Check - Duplicate orders: {duplicate_counts}")

# Step 2: Customer uniqueness analysis
customers = df.select("CustomerName", "Email")
print(f"üìä Total customer records: {customers.count()}")
print(f"üéØ Unique customers: {customers.distinct().count()}")

# Step 3: Complete customer analysis pipeline
marketing_analysis = df.withColumn(
    "ProductCategory", categorize_product_udf(col("Item"))
).withColumn(
    "Revenue", col("UnitPrice") * col("Quantity")
).withColumn(
    "OrderYear", year(col("OrderDate"))
)

# Customer behavior metrics
customer_insights = marketing_analysis.groupBy("Email", "CustomerName").agg(
    sum("Revenue").alias("TotalRevenue"),
    count("SalesOrderNumber").alias("TotalOrders"),
    countDistinct("Item").alias("ProductVariety"),
    countDistinct("OrderYear").alias("ActiveYears"),
    avg("UnitPrice").alias("AvgOrderValue")
).withColumn(
    "MarketingScore",
    loyalty_score_udf(col("TotalRevenue"), col("TotalOrders"), col("ProductVariety"))
).withColumn("TargetSegment",
    when(col("MarketingScore") >= 80, "High-Value Targets")
    .when(col("MarketingScore") >= 60, "Growth Potential")
    .when(col("MarketingScore") >= 40, "Retention Focus")
    .otherwise("Re-engagement Needed")
)

# Create temp view for SQL analysis
customer_insights.createOrReplaceTempView("customer_insights")
marketing_analysis.createOrReplaceTempView("marketing_data")

print("\nüìä MARKETING SEGMENT ANALYSIS:")
display(customer_insights.groupBy("TargetSegment").agg(
    count("*").alias("CustomerCount"),
    sum("TotalRevenue").alias("SegmentValue"),
    avg("MarketingScore").alias("AvgScore")
).orderBy(desc("SegmentValue")))

# COMMAND ----------

# Advanced SQL analysis combining multiple concepts
# MAGIC %sql
# MAGIC SELECT
# MAGIC   ci.TargetSegment,
# MAGIC   md.ProductCategory,
# MAGIC   sum(md.Revenue) as CategoryRevenue,
# MAGIC   count(*) as TransactionCount,
# MAGIC   avg(md.UnitPrice) as AvgPrice,
# MAGIC   count(distinct md.Email) as UniqueCustomers
# MAGIC FROM marketing_data md
# MAGIC JOIN customer_insights ci ON md.Email = ci.Email
# MAGIC GROUP BY ci.TargetSegment, md.ProductCategory
# MAGIC HAVING sum(md.Revenue) > 10000
# MAGIC ORDER BY ci.TargetSegment, CategoryRevenue DESC

# COMMAND ----------

print("üéØ FINAL MARKETING RECOMMENDATIONS:")
print("‚úÖ Focus on 'High-Value Targets' - highest revenue potential")
print("‚úÖ Mountain-Premium bikes drive the most revenue")
print("‚úÖ Develop retention programs for mid-tier customers")
print("‚úÖ Create re-engagement campaigns for dormant customers")
print("‚úÖ B2B customers show different buying patterns than consumers")
```

---

## üèÜ DATABRICKS-OPTIMIZED CHECKLIST

### ‚úÖ Databricks Best Practices Applied

- [x] **No SparkSession initialization** - Databricks handles this automatically
- [x] **Use `display()` instead of `show()`** - Better visualizations and interactivity
- [x] **Magic commands** - `%sql` for SQL blocks, `%python` for Python
- [x] **DBFS file paths** - `/FileStore/` for data storage
- [x] **Schema definition** - `StructType` for consistent data loading
- [x] **Temp views** - `createOrReplaceTempView()` for SQL analysis

### ‚úÖ Performance Optimizations

- [x] **Load all files at once** - `*.csv` wildcard pattern
- [x] **Built-in functions over UDFs** - When possible for speed
- [x] **Efficient filtering** - Early filter operations
- [x] **Proper data types** - Schema enforcement prevents errors

### ‚úÖ Advanced Features

- [x] **SQL Magic** - Seamless SQL and Python integration
- [x] **Custom UDFs** - Complex business logic implementation
- [x] **Window functions** - Advanced analytics capabilities
- [x] **Pivot operations** - Data transformation for reporting

---

## üöÄ NEXT STEPS: Advanced Databricks Features

Ready for more advanced Databricks capabilities?

1. **Delta Lake** - ACID transactions and time travel
2. **Auto Loader** - Streaming file ingestion
3. **MLflow** - Machine learning lifecycle management
4. **Databricks SQL** - Business intelligence and dashboards
5. **Jobs & Workflows** - Production data pipelines

**You now have production-ready PySpark skills optimized for Azure Databricks!** üéâ
